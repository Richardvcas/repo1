Created by Woolsgrove, Si, last updated by Medisetti, Arjuna 25 minutes ago  3 minute read
To allow us to modify the guest we can use cloud-init userdata (Note: ICPW vmware uses guestinfo parameters, no longer required with Openshift)



cloud-init formats
cloud-init can be passed in a number of ways to the guest, you can read more here https://kubevirt.io/user-guide/user_workloads/startup_scripts/#cloud-init 



cloud-init as a secret
Create your cloud-init file, most of the data here should be unique to each guest VM. Please note some of the stuff here under runcmd will go to image so that it will be cleaner.



cat wh-labocp-002-cloud-init.yaml
#cloud-config
runcmd:
  - 'set -x'
  - 'sed -i "/# source INSTALL_CLASS/i generate_fbenv" /usr/local/bin/fb-init.sh' 
  - 'cat /etc/INSTALL_CLASS.new >> /etc/INSTALL_CLASS'
  - '[[ `/usr/bin/cloud-id` == "nocloud" ]] && [[ -f "/usr/local/bin/fb-init.sh" ]] && /usr/local/bin/fb-init.sh'
  - 'jpmc-dnf-config configure --get-content-hostname > /tmp/jpmc-dnf-config-original-content-hostname'
  # lab only
  # ---------
  - 'jpmc-dnf-config configure --set-content-hostname=repomirror-lab.jpmchase.net'
  # set lab DNF mirrorlist hostname aliases
  - "for host in `curl https://repomirror-lab.jpmchase.net/pulp/content/dnf/sources/lrh9/baseos-x86_64.mirrorlist | awk -F/ '{print $3}'`; do echo 10.240.47.188 $host >> /etc/hosts;done"
  # reload dnsmasq to pick up the new /etc/hosts entries
  - 'pgrep dnsmasq && kill -HUP $(pgrep dnsmasq)'
  # everything else
  - 'yum install -y sudo'
# allow password auth
ssh_pwauth: true
users:
  - name: root
    # mkpasswd -m sha-512 --rounds=6
    # m-----1
    hashed_passwd: $6$rounds=1000$DiGeA4uGWXSuaEz9$KyunUR3wwSwIarSxHk6MXmgpXUVZIknrsyCPYnHrZMg2R9PCxr0XYxqi.Ig6IUZXORupSs2hp46FhJOb2bnjl1
  - name: wilmauser
    gecos: Wilma User
    sudo: ["ALL=(ALL) NOPASSWD:ALL"]
    ssh-authorized-keys:
      - <<< YOUR SSH KEY HERE >>>
write_files:
  - path: /etc/INSTALL_CLASS.new
    content: |
      KSfbuser="root"
      KSfbhostname="wh-labocp-002.wh2.lab.jpmchase.net"
      KSfbfqdn="true"
      KSfbip="110.14.221.132"
      KSfbnetmask="255.255.255.192"
      KSfbgateway="10.14.221.129"
      KSfbdnsns="10.240.47.3 10.240.41.3"
      KSfbdnssearch="wh2.lab.jpmchase.net"
      KSlob="GTI"
      KSregion="Brooklyn"
      KSsite="lab1"
      KSnwzone="lab"
      timezone="UTC"
      supportTeam="GDEALINUX"
      KSicpenv="prod"
      KSenv="dev"
      KSautomation="icpw"
      KSgredl="00057"
      supportTeam="GSO SRVR UNIX"
      icp_boot_config="false"
      icp_boot_config_icp_puppet="true"
      icp_boot_config_icp_puppet_build_script="puppet_run.rb"
      icp_boot_config_icp_evolven="true"
      icp_boot_config_icp_powerbroker="true"
      icp_boot_config_icp_tivoli_itm6="true"
      icp_boot_config_icp_aim="true"
      icp_boot_config_icp_fluentd="true"
      KScohesity="auto"
      icp_boot_config_icp_crowdstrike="true"
      icp_boot_config_icp_cida="true"
      icp_boot_config_icp_cida_node_type="standard"
      icp_boot_config_icp_cida_template="ucm_auth_security_cida"
      icp_boot_config_icp_cida_addomain="exchadlab.gielab.jpmchase.net"
      epv_require_onboarding="false"
      epv_repave="true"
      epv_accountType="Shared Interactive"
      epv_platform="UnixStandalone - Sophia - Root"
      epv_accountName="root"
      epv_region="Global"
      epv_subRegion="Global"
      epv_networkLocation="LAN"
      epv_neimId="G024162"
      epv_locationId="00057"
      epv_accessGroupName=""
      sealId="28954"
      icp_boot_config_remove_root_pub_keys="true"
hostname: wh-labocp-002
fqdn: wh-labocp-002.wh2.lab.jpmchase.net
manage_etc_hosts: true



Create secret using above userdata



oc create secret generic wh-labocp-002-cloud-init --from-file=userdata=wh-labocp-002-cloud-init.yaml -n linuxeng




Apply vm definition , make sure to update Network section specific to Guest VM



Full Example


apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  labels:
    kubevirt.io/vm: wh-labocp-002
  name: wh-labocp-002
spec:
  dataVolumeTemplates:
  - metadata:
      name: vm-fred3-dv1
    spec:
      storage:
        resources:
          requests:
            storage: 110Gi
        # storageClassName: ocs-storagecluster-ceph-rbd
        storageClassName: ocs-storagecluster-ceph-rbd-virtualization
      source:
        http:
          url: http://fred11420.gielab.jpmchase.net/images/img_ovlt_rhel_9_9.6.1_2025-07-09-100051.qcow2
            #registry:
            #url: docker://fred31420.gielab.jpmchase.net:8443/jpmc/ovlt_rhel_9:9.6.1_2025-07-09-100051
            #secretRef: fred31420-data-source-secret
            #certConfigMap: fred31420-tls-certs
    status: {}
  running: true
  template:
    metadata:
      labels:
        kubevirt.io/vm: wh-labocp-002
    spec:
      domain:
        firmware:
          bootloader:
            efi:
              secureBoot: false
        devices:
          interfaces:
            - name: nic-gray-wh-labocp-002
              bridge: {}
          disks:
          - disk:
              bus: virtio
            name: vm-fred3-dv1
          - disk:
              bus: virtio
            name: cloudinitdisk
        machine:
          type: ""
        cpu:
          cores: 2
        resources:
          requests:
            memory: 8Gi
      terminationGracePeriodSeconds: 180
      networks:
        - name: nic-gray-wh-labocp-002
          multus:
            networkName: default/vlan3702
      volumes:
      - dataVolume:
          name: vm-fred3-dv1
        name: vm-fred3-dv1
      - name: cloudinitdisk
        cloudInitNoCloud:
          secretRef:
            name: wh-labocp-002-cloud-init
          networkData: |
            version: 2
            ethernets:
              eth0:
                dhcp4: false
                addresses:
                  - 10.14.221.132/26
                gateway4: 10.14.221.129
                nameservers:
                  addresses:
                    - 10.240.47.3
                    - 10.240.41.3


Once VM is up, you should be able to ssh and run ansible playbook to configure agents.

Create local playbook and run (only for LAB), 

From lab:



$cat /run_icp_boot_config.yml

- name: Run icp_boot_config
  hosts: "{{ host }}"
  gather_facts: no

  tasks:
    - name: Wait for cloud-init config to complete & permit ssh connection
      ansible.builtin.wait_for_connection:
        delay: 30
        timeout: 10

    - name: Check if icp_boot_config exists
      stat:
        path: /var/tmp/icp_boot_config
      register: stat_icp_boot_config
      
    - name: Run icp_boot_config
      become: true
      shell: /usr/bin/ansible-playbook -v /var/tmp/icp_boot_config/icp_boot_config.yml
      async: 2700
      poll: 60
      register: icp_boot_config_result
      failed_when: icp_boot_config_result.rc is not defined  # no .rc attribute if async timeout is reached (also stdout and stderr will be blank)
      when: stat_icp_boot_config.stat.exists

    - name: stdout of icp_boot_config
      debug:
        msg: "{{ icp_boot_config_result.stdout.split('\n') }}"
      when: stat_icp_boot_config.stat.exists

    - name: stderr of icp_boot_config
      debug:
        msg: "{{ icp_boot_config_result.stderr.split('\n') }}"
      when: stat_icp_boot_config.stat.exists

    - name:
      fail:
       msg: "FATAL Run of icp_boot_config failed"
      when: stat_icp_boot_config.stat.exists and icp_boot_config_result.rc != 0



remote exec - currently puppet is working, sophia is waiting for registration, crowdstrike requires proxy access



ansible-playbook /var/tmp/run_icp_boot_config.yml -i wh-labocp-002.wh2.lab.jpmchase.net, --private-key=~/.ssh/kvm_rsa --user=wilmauser --become --become-method=sudo --become-user=root --limit=wh-labocp-002.wh2.lab.jpmchase.net --extra-vars "host=wh-labocp-002.wh2.lab.jpmchase.net"


For outside the lab, may need to leverage ICPW remote exec process or similar.

Create ssh process and run 



ansible-playbook /var/tmp/icp_boot_config/icp_boot_config.yml --connection=local

logs will be under /root/agent-build.logs
Agent Logging is important for support, in the future we can leverage cloud events for each agent if possible.


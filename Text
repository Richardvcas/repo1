cloud-init formats
cloud-init can be passed in a number of ways to the guest, you can read more here https://kubevirt.io/user-guide/user_workloads/startup_scripts/#cloud-init 



cloud-init as a secret
Create your cloud-init file, most of the data here should be unique to each guest VM. Please note some of the stuff here under runcmd will go to image so that it will be cleaner.



cat wh-labocp-002-cloud-init.yaml
#cloud-config
runcmd:
  - 'set -x'
  - 'sed -i "/# source INSTALL_CLASS/i generate_fbenv" /usr/local/bin/fb-init.sh' 
  - 'cat /etc/INSTALL_CLASS.new >> /etc/INSTALL_CLASS'
  - '[[ `/usr/bin/cloud-id` == "nocloud" ]] && [[ -f "/usr/local/bin/fb-init.sh" ]] && /usr/local/bin/fb-init.sh'
  - 'jpmc-dnf-config configure --get-content-hostname > /tmp/jpmc-dnf-config-original-content-hostname'
  # lab only
  # ---------
  - 'jpmc-dnf-config configure --set-content-hostname=repomirror-lab.jpmchase.net'
  # set lab DNF mirrorlist hostname aliases
  - "for host in `curl https://repomirror-lab.jpmchase.net/pulp/content/dnf/sources/lrh9/baseos-x86_64.mirrorlist | awk -F/ '{print $3}'`; do echo 10.240.47.188 $host >> /etc/hosts;done"
  # reload dnsmasq to pick up the new /etc/hosts entries
  - 'pgrep dnsmasq && kill -HUP $(pgrep dnsmasq)'
  # everything else
  - 'yum install -y sudo'
# allow password auth
ssh_pwauth: true
users:
  - name: root
    # mkpasswd -m sha-512 --rounds=6
    # m-----1
    hashed_passwd: $6$rounds=1000$DiGeA4uGWXSuaEz9$KyunUR3wwSwIarSxHk6MXmgpXUVZIknrsyCPYnHrZMg2R9PCxr0XYxqi.Ig6IUZXORupSs2hp46FhJOb2bnjl1
  - name: wilmauser
    gecos: Wilma User
    sudo: ["ALL=(ALL) NOPASSWD:ALL"]
    ssh-authorized-keys:
      - <<< YOUR SSH KEY HERE >>>
write_files:
  - path: /etc/INSTALL_CLASS.new
    content: |
      KSfbuser="root"
      KSfbhostname="wh-labocp-002.wh2.lab.jpmchase.net"
      KSfbfqdn="true"
      KSfbip="110.14.221.132"
      KSfbnetmask="255.255.255.192"
      KSfbgateway="10.14.221.129"
      KSfbdnsns="10.240.47.3 10.240.41.3"
      KSfbdnssearch="wh2.lab.jpmchase.net"
      KSlob="GTI"
      KSregion="Brooklyn"
      KSsite="lab1"
      KSnwzone="lab"
      timezone="UTC"
      supportTeam="GDEALINUX"
      KSicpenv="prod"
      KSenv="dev"
      KSautomation="icpw"
      KSgredl="00057"
      supportTeam="GSO SRVR UNIX"
      icp_boot_config="false"
      icp_boot_config_icp_puppet="true"
      icp_boot_config_icp_puppet_build_script="puppet_run.rb"
      icp_boot_config_icp_evolven="true"
      icp_boot_config_icp_powerbroker="true"
      icp_boot_config_icp_tivoli_itm6="true"
      icp_boot_config_icp_aim="true"
      icp_boot_config_icp_fluentd="true"
      KScohesity="auto"
      icp_boot_config_icp_crowdstrike="true"
      icp_boot_config_icp_cida="true"
      icp_boot_config_icp_cida_node_type="standard"
      icp_boot_config_icp_cida_template="ucm_auth_security_cida"
      icp_boot_config_icp_cida_addomain="exchadlab.gielab.jpmchase.net"
      epv_require_onboarding="false"
      epv_repave="true"
      epv_accountType="Shared Interactive"
      epv_platform="UnixStandalone - Sophia - Root"
      epv_accountName="root"
      epv_region="Global"
      epv_subRegion="Global"
      epv_networkLocation="LAN"
      epv_neimId="G024162"
      epv_locationId="00057"
      epv_accessGroupName=""
      sealId="28954"
      icp_boot_config_remove_root_pub_keys="true"
hostname: wh-labocp-002
fqdn: wh-labocp-002.wh2.lab.jpmchase.net
manage_etc_hosts: true



Create secret using above userdata



oc create secret generic wh-labocp-002-cloud-init --from-file=userdata=wh-labocp-002-cloud-init.yaml -n linuxeng




Apply vm definition , make sure to update Network section specific to Guest VM



Full Example


apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  labels:
    kubevirt.io/vm: wh-labocp-002
  name: wh-labocp-002
spec:
  dataVolumeTemplates:
  - metadata:
      name: vm-fred3-dv1
    spec:
      storage:
        resources:
          requests:
            storage: 110Gi
        # storageClassName: ocs-storagecluster-ceph-rbd
        storageClassName: ocs-storagecluster-ceph-rbd-virtualization
      source:
        http:
          url: http://fred11420.gielab.jpmchase.net/images/img_ovlt_rhel_9_9.6.1_2025-07-09-100051.qcow2
            #registry:
            #url: docker://fred31420.gielab.jpmchase.net:8443/jpmc/ovlt_rhel_9:9.6.1_2025-07-09-100051
            #secretRef: fred31420-data-source-secret
            #certConfigMap: fred31420-tls-certs
    status: {}
  running: true
  template:
    metadata:
      labels:
        kubevirt.io/vm: wh-labocp-002
    spec:
      domain:
        firmware:
          bootloader:
            efi:
              secureBoot: false
        devices:
          interfaces:
            - name: nic-gray-wh-labocp-002
              bridge: {}
          disks:
          - disk:
              bus: virtio
            name: vm-fred3-dv1
          - disk:
              bus: virtio
            name: cloudinitdisk
        machine:
          type: ""
        cpu:
          cores: 2
        resources:
          requests:
            memory: 8Gi
      terminationGracePeriodSeconds: 180
      networks:
        - name: nic-gray-wh-labocp-002
          multus:
            networkName: default/vlan3702
      volumes:
      - dataVolume:
          name: vm-fred3-dv1
        name: vm-fred3-dv1
      - name: cloudinitdisk
        cloudInitNoCloud:
          secretRef:
            name: wh-labocp-002-cloud-init
          networkData: |
            version: 2
            ethernets:
              eth0:
                dhcp4: false
                addresses:
                  - 10.14.221.132/26
                gateway4: 10.14.221.129
                nameservers:
                  addresses:
                    - 10.240.47.3
                    - 10.240.41.3


Once VM is up, you should be able to ssh and run ansible playbook to configure agents.

Create local playbook and run (only for LAB), 

From lab:

Note: remote exec - currently puppet and sophia are working, crowdstrike requires proxy access



ssh -t -o StrictHostKeyChecking=no root@wh-labocp-002.wh2.lab.jpmchase.net -i ~/.ssh/kvm_rsa 'ansible-playbook /var/tmp/icp_boot_config/icp_boot_config.yml --connection=local'
